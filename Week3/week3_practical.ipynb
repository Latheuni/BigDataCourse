{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## general packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session 3: Disciminant analysis and classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2>Table of Contents </h2><a class=\"anchor\" id=\"top\"></a>\n",
    "    <br><a href=\"#section_1\"> Linear Disciminant analysis</a>\n",
    "    <br><a href=\"#intermezzo\">Intermezzo - Hyperparameter optimization </a>\n",
    "    <br><a href=\"#section_2\">Support vector machines (SVM) </a>\n",
    "    <br><a href=\"#Optional\">Optional: LDA from scratch</a>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\"> Linear Disciminant analysis (LDA) </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_1\"></a> \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "Due to time limits during the practical sessions, it is not possible to go super in depth about the models. For the LDA an additional section is added at the bottom of the notebook that covers the formula's seen during the lecture and let's you implement an LDA classifier yourself on a very simple toy dataset. Feel free to first start with that part if you wan to. During these practicals the main goal is clarify the theory and teach you how to implement the models yourself. So please go through the notebooks at your own pace, focussing on what you find most interesting. </div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "For reproducibility purposes we will always use random_state=0 in this practical. Don't forget to assign this parameter in your functions if it is needed.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3> Exercise:</h3>\n",
    "<ol> \n",
    " <li> Use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\">iris dataset</a> from last lab, load the X and y data in the notebook\n",
    " <li> Assess the assumptions the LDA classifier enforces on the predictors with the help of <b>the guide code</b> down below. \n",
    "    <ol> \n",
    "        <li> <u>For the normality assumption</u>: check the distributions of the predictors with histogram plots and select the predictors that conform to the normality assumption.\n",
    "        <li> <u>For the equal variance assumption</u>: check the variances of the chosen predictors with boxplots (if more than one is chosen) and perform a data transformation if necessary.\n",
    "    </ol>\n",
    " <li> Split the data 80/20 in a train/test set (Note when only one regressor is selected, it should be correctly represented (not as a row but as a column vector. Use .values.reshape(-1,1) to fix this issue).\n",
    " <li> Construct <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\">the LDA classifier</a>.\n",
    " <li> Fit the classifier, predict the test labels and calculate the accuracy score of the prediction task.\n",
    " </ol>\n",
    "</div>\n",
    "\n",
    "**guide-code:**\n",
    "```\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "sns.histplot(predictor1, kde = True, color=\"skyblue\", ax = axes[0,0])\n",
    "sns.histplot(predictor2, kde = True, color=\"skyblue\", ax = axes[0,1])\n",
    "...\n",
    "\n",
    "\n",
    "sns.boxplot(data, color=\"skyblue\", ax = axes[0,0])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "...\n",
    "\n",
    "# Load in data\n",
    "iris_X, iris_y = ...\n",
    "\n",
    "# Inspect data\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect normality\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect equal variance\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9666666666666667\n",
      "Accuracy score 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Construct the training and test sets\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# Fit and assess the model\n",
    "...\n",
    "\n",
    "print('Accuracy score', ...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\"> Intermezzo - Hyperparameter optimization </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"intermezzo\"></a> \n",
    "\n",
    "Most models, including support vector machines that will be covered in the next sections, have tunable hyperparameters. Hyperparameters are parameters whose values control the learning process and determine the values of model parameters that a learning algorithm ends up learning. In the case of the SVM, the main hyperparameter is the regularization parameter (the lambda added before the norm in the regularization norm).  But the kernel itself can also be supplied as a hyperparameter. \n",
    "\n",
    "Two searches are often proposed to tune a machine learning model in function of its hyperparameters: \n",
    "- the first considers an exhaustive and structured search in the full hyperparameter space. \n",
    "- the second applies random combinations of hyperparameter settings. \n",
    "\n",
    "![grid](./grid_vs_randomgrid.png)\n",
    "\n",
    "[It has been shown that when only few hyperparameters have an impact on model performance a randomized grid search is preferred. ](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) \n",
    "\n",
    "`Scikit-learn` offers implementations of [both](http://scikit-learn.org/stable/modules/grid_search.html) within a cross-validation scheme for hyperparameter tuning (see slide 36 of Lecture 2a on nested cross-validation).\n",
    "\n",
    "![CV](./slide36.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\"> Support vector machines (SVM) </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_2\"></a> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Exercise: \n",
    "<ol>\n",
    "<li> Choose one of the datasets down below to continue the exercise with.\n",
    "<li> Perform an 80/20 train-test split.\n",
    "<li> Define a hyperparameter dictionary with multiple values of the 'C' parameter in the SVC model and a linear and rbf kernel (hint look at the documentation of the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">GridSearchCV</a> or <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\">RandomSearchCV</a> to  find the exact syntax).\n",
    "<li> Implement the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">GridSearchCV</a> or the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\">RandomSearchCV</a> (or both ðŸ˜‰), fit the constructor and make predictions.\n",
    "<li> Look at the optimal hyperparameters selected and evaluate your predictions with <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\">the accuracy score metric</a>. \n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## Dataset 1 ##\n",
    "###############\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "X_, y_= make_circles(n_samples=500, noise=0.06, random_state=42)\n",
    "X_circle, y_circle = pd.DataFrame(X_), pd.DataFrame(y_)\n",
    "\n",
    "# Uncomment to check the content of the data\n",
    "#X_circle.head()\n",
    "\n",
    "# Plot the dataset\n",
    "df = pd.DataFrame(dict(x1=X_[:, 0], x2=X_[:, 1], y=y_))\n",
    "colors = {0:'blue', 1:'yellow'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('y')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x1', y='x2', label=key, color = colors[key])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## Dataset 2 ##\n",
    "###############\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X_cancer, y_cancer= load_breast_cancer(return_X_y = True, as_frame = True)\n",
    "\n",
    "# Uncomment to check the content of the data\n",
    "#X_cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) use the time module to find out how long the calculations take, time() returns the current time\n",
    "# from time import time\n",
    "\n",
    "# Perform the train/test splits\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# Define the parameter dictionary\n",
    "parameters = {\n",
    "    ...\n",
    "}\n",
    "\n",
    "# Initialize the RandomizeSearchCV or the GridSearchCV\n",
    "...\n",
    "\n",
    "# Fit the object and make predictions\n",
    "...\n",
    "\n",
    "# Calculate the accuracy score\n",
    "acc = ...\n",
    "print('Accuracy score on the ?? with ??CV', acc)\n",
    "\n",
    "# (Optional) print the lapsed time\n",
    "...\n",
    "\n",
    "# Extract and inspect the best parameters from the CV object\n",
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ```cv_results_```will give a lot more information on the GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\"> Optional: LDA from scratch </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"Optional\"></a> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the binary classification setting, we want to predict the probability that an instance belongs to one of two classes, given the data: $p(y|\\mathbf{x})$, also called the class posterior. In the previous lab, we saw that logistic regression directly models this probability. Classifiers that directly model the class posterior are called **discriminative classifiers**. Logistic regression is one of the most popular discriminative classification algorithms. \n",
    "\n",
    "In contrast, **generative classifiers** first model the probability of the data for each specific class $p(\\mathbf{x}|y)$ and then use Bayes' rule to invert that probability to obtain the class posterior:\n",
    "\n",
    "\\begin{equation}\n",
    "Pr(y=k|\\mathbf{x}) = \\frac{Pr(\\mathbf{x}|y=k)Pr(y=k)}{Pr(\\mathbf{x})} = \\frac{Pr(\\mathbf{x}|y=k)Pr(y=k)}{\\sum_{l=1}^{K}Pr(\\mathbf{x}|y=l)Pr(y=l)}\n",
    "\\end{equation}\n",
    "\n",
    "These models are called generative because they specify the class-conditional densities $p(\\mathbf{x}|y)$, which allows to generate new observations $\\mathbf{x}$ for a class $y$: for a given class $y$, we can sample from the distribution $p(\\mathbf{x}|y)$. \n",
    "\n",
    "**LDA is an example of a generative classifier.** LDA assumes that the data in each class can be modelled as a multivariate Gaussian distribution. Furthermore, the assumption is made that the (co)variance within and between predictor variables is the same for all the classes. As such, only a limited number of parameters have to be estimated from the data to fully specify the class-conditional distributions: in the multivariate case, we need to estimate the class-specific mean vectors $\\mathbf{\\mu_{k}}$ and the common covariance matrix $\\Sigma$. In the case of just one predictor, this simplifies to estimating the mean of $x$ within each class and the variance $\\sigma^2$. The priors $p(y=k)$ are simply estimated from the data by calculating the proportion of training instances in each class. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To introduce the LDA, we will make use of a simple 1-dimensional toy dataset that contains two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.418646</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.485199</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.983165</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.732385</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.291976</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x    y\n",
       "0  2.418646  1.0\n",
       "1  0.485199  1.0\n",
       "2  0.983165  1.0\n",
       "3  3.732385  1.0\n",
       "4  0.291976  1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data\n",
    "data = pd.read_csv('toydata_LDA.csv')\n",
    "type(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train/test sets following a 90/10 ratio\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# The code down below will visualize the data and the train-test split\n",
    "# (The y-axis contains just random scatter as the toy data is 1D)\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.scatter(X_train, np.random.normal(scale=0.001,size=len(X_train)),\n",
    "           color=['#b2182b' if y else '#2166ac' for y in y_train],\n",
    "          edgecolor='black')\n",
    "ax.get_yaxis().set_ticks([]);\n",
    "ax.scatter(X_test, np.random.normal(scale=0.001, size=len(X_test)),\n",
    "          color='white', edgecolor=['#b2182b' if y else '#2166ac' for y in y_test], linewidth=3, s=50)\n",
    "ax.legend(['Training data', 'Test data'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to classify the test data in white with LDA. In order to do so, we will need to estimate the prior probabilities of an observation belonging to each class and the class-conditional densities. In LDA, we make a convenient assumption: we model the class-conditional densities with a normal distribution. In that way, we only need to estimate the mean and the variance for each class. Furthermore, we assume that the variance is the same in both classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3> Exercise </h3>  \n",
    "<p>\n",
    "Using the training data, estimate the parameters that are required for LDA. The priors $p(y=k)$ are simply estimated from the data by calculating the proportion of training instances in each class.To estimate the variance, use the following formula (see slide 6 of the theory lectures), where $K$ denotes the number of classes: \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "![formula](./slide6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the prior probabilities\n",
    "p_0 = ...\n",
    "p_1 = ...\n",
    "\n",
    "# Estimate the mean of both gaussians\n",
    "mu_0 = ...\n",
    "mu_1 = ...\n",
    "\n",
    "# Estimate the variance sigmaÂ²\n",
    "sigma_sq = ...\n",
    "\n",
    "print('p_0: ' + str(p_0))\n",
    "print('p_1: ' + str(p_1))\n",
    "print('mu_0: ' + str(mu_0))\n",
    "print('mu_1: ' + str(mu_1))\n",
    "print('sigma_sq: ' + str(sigma_sq)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Exercise</h3>\n",
    "<p>\n",
    "Now, for each instance $x$ in the test data set, use the parameters above to fill in Bayes' rule and to calculate the probability of that instance to belong to each class. Use scipy.stats.norm.pdf(x, location, scale) to calculate the pdf of the normal distribution. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sc \n",
    "\n",
    "prob_class_0 = []\n",
    "prob_class_1 = []\n",
    "\n",
    "for x in X_test:\n",
    "    likelihood_0 = ...\n",
    "    likelihood_1 = ...\n",
    "    marginal = p_0*likelihood_0 + p_1*likelihood_1\n",
    "    \n",
    "    posterior_0 = p_0*likelihood_0/marginal\n",
    "    posterior_1 = p_1*likelihood_1/marginal\n",
    "    prob_class_0.append(posterior_0)\n",
    "    prob_class_1.append(posterior_1)\n",
    "\n",
    "# ** solution **\n",
    "predicted_classes = [0 if prob_class_0[i] > prob_class_1[i] else 1 for i in range(len(prob_class_0))]\n",
    "\n",
    "accuracy = np.round(np.mean(predicted_classes == y_test)*100,2)\n",
    "print('Classification accuracy: {}%'.format(accuracy))\n",
    "\n",
    "# Visualize the predictions\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.scatter(X_train, np.random.normal(scale=0.001,size=len(X_train)),\n",
    "           color=['#b2182b' if y else '#2166ac' for y in y_train],\n",
    "          edgecolor='black')\n",
    "ax.get_yaxis().set_ticks([]);\n",
    "test_jitter = np.random.normal(scale=0.001, size=len(X_test))\n",
    "ax.scatter(X_test, test_jitter,\n",
    "          color='white', edgecolor=['#b2182b' if y else '#2166ac' for y in y_test], s=100, linewidth=3)\n",
    "ax.scatter(X_test, test_jitter,\n",
    "          color=['#b2182b' if y else '#2166ac' for y in predicted_classes], s=20)\n",
    "ax.set_title('Toy dataset - LDA predictions. Accuracy: {}'.format(accuracy)).set_fontsize(20)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "        <p>\n",
    "The two classes from the toy dataset were simulated from two Gaussian distributions with mean -1.5 for class 0 and 1.5 for class 1, and a standard deviation of 1.2. Run the code below to see how well LDA has modelled these distributions. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    xrange = np.arange(-5,5,0.01)\n",
    "    dist_0 = sc.stats.norm.pdf(xrange, loc=-1.5, scale=1.2)\n",
    "    dist_1 = sc.stats.norm.pdf(xrange, loc=1.5, scale=1.2)\n",
    "    est_0 = sc.stats.norm.pdf(xrange, loc=mu_0, scale=sigma_sq**0.5)\n",
    "    est_1 = sc.stats.norm.pdf(xrange, loc=mu_1, scale=sigma_sq**0.5)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    ax.scatter(xrange, dist_0, color='#2166ac', marker='.')\n",
    "    ax.scatter(xrange, dist_1, color='#b2182b', marker='.')\n",
    "    ax.scatter(xrange, est_0, color='#92c5de', marker='.')\n",
    "    ax.scatter(xrange, est_1, color='#d6604d', marker='.')\n",
    "\n",
    "    # Add the training points \n",
    "    ax.scatter(X_train, np.random.normal(loc=-0.1,scale=0.005,size=len(X_train)),\n",
    "           color=['#b2182b' if y else '#2166ac' for y in y_train],\n",
    "          edgecolor='black')\n",
    "    ax.get_yaxis().set_ticks([]);\n",
    "    ax.legend(['True distribution of class 0',\n",
    "            'True distribution of class 1', \n",
    "            'Estimated pdf of class 0',\n",
    "            'Estimated pdf of class 1'],\n",
    "             fontsize=14)\n",
    "except:\n",
    "    print('Estimate the parameters of the Gaussians first: mu_0, mu_1, sigma_sq')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5ad70b114478dbfeb7686dda59c481057deed804443829ded50851cf484fa5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
