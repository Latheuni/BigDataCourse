{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session 3: Disciminant analysis and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2>Table of Contents </h2><a class=\"anchor\" id=\"top\"></a>\n",
    "    <br><a href=\"#section_1\"> Linear Disciminant analysis</a>\n",
    "    <br><a href=\"#intermezzo\">Intermezzo - Hyperparameter optimization </a>\n",
    "    <br><a href=\"#section_2\">Support vector machines (SVM) </a>\n",
    "    <br><a href=\"#Optional\">Optional: LDA from scratch</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\"> Linear Disciminant analysis (LDA) </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_1\"></a> \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "Due to time limits during the practical sessions, it is not possible to go really in depth about the models. For the LDA, an additional section is added at the bottom of the notebook that covers the formulas seen during the lecture and lets you implement an LDA classifier yourself on a very simple toy dataset. Feel free to first start with that part if you want to. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "For reproducibility purposes we will always use random_state=0 in this practical. Don't forget to assign this parameter in your functions if it is needed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3> Exercise:</h3>\n",
    "<ol> \n",
    " <li> Use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\">iris dataset</a> from last lab, load the X and y data in the notebook\n",
    " <li> Assess the assumptions the LDA classifier enforces on the predictors with the help of <b>the guide code</b> down below. \n",
    "    <ol> \n",
    "        <li> <u>For the normality assumption</u>: check the distributions of the predictors with histogram plots and select the predictors that conform to the normality assumption.\n",
    "        <li> <u>For the equal variance assumption</u>: check the variances of the chosen predictors with boxplots (if more than one is chosen) and perform a data transformation if necessary.\n",
    "    </ol>\n",
    " <li> Split the data 80/20 in a train/test set (Note when only one regressor is selected, it should be correctly represented (not as a row but as a column vector. Use <code>.values.reshape(-1,1)</code> to fix this issue).\n",
    " <li> Construct <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\">the LDA classifier</a>.\n",
    " <li> Fit the classifier, predict the test labels and calculate the accuracy score of the prediction task.\n",
    " </ol>\n",
    "</div>\n",
    "\n",
    "**guide code:**\n",
    "```\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "sns.histplot(predictor1, kde = True, color=\"skyblue\", ax = axes[0,0])\n",
    "sns.histplot(predictor2, kde = True, color=\"skyblue\", ax = axes[0,1])\n",
    "...\n",
    "\n",
    "\n",
    "sns.boxplot(data, color=\"skyblue\", ax = axes[0,0])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "# Load in data\n",
    "iris_X, iris_y = datasets.load_iris(return_X_y = True, as_frame = True)\n",
    "\n",
    "# Inspect data\n",
    "iris_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='petal width (cm)', ylabel='Count'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect normality\n",
    "fig, axes = plt.subplots(2, 2, figsize = (10,10))\n",
    "sns.histplot(iris_X.iloc[:,0], kde = True, color=\"skyblue\", ax = axes[0,0])\n",
    "sns.histplot(iris_X.iloc[:,1], kde = True, color=\"skyblue\", ax = axes[1,0])\n",
    "sns.histplot(iris_X.iloc[:,2], kde = True, color=\"skyblue\", ax = axes[0,1])\n",
    "sns.histplot(iris_X.iloc[:,3], kde = True, color=\"skyblue\", ax = axes[1,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NormaltestResult(statistic=5.735584236235733, pvalue=0.05682424941067306)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively you can use normaltest from scipy\n",
    "from scipy.stats import normaltest\n",
    "normaltest(iris_X.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'petal width (cm)')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect equal variance\n",
    "fig, axes = plt.subplots(2, 2, figsize = (10,10))\n",
    "sns.boxplot(data=np.log(iris_X.iloc[:,0]), color=\"skyblue\", ax = axes[0,0])\n",
    "axes[0,0].set_title(iris_X.columns[0])\n",
    "sns.boxplot(data=np.log(iris_X.iloc[:,1]), color=\"skyblue\", ax = axes[1,0])\n",
    "axes[1,0].set_title(iris_X.columns[1])\n",
    "sns.boxplot(data=np.log(iris_X.iloc[:,2]), color=\"skyblue\", ax = axes[0,1])\n",
    "axes[0,1].set_title(iris_X.columns[2])\n",
    "sns.boxplot(data=np.log(iris_X.iloc[:,3]), color=\"skyblue\", ax = axes[1,1])\n",
    "axes[1,1].set_title(iris_X.columns[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Construct the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_X.iloc[:,2].values.reshape(-1,1), iris_y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Fit and assess the model\n",
    "sklearn_lda = LDA()\n",
    "sklearn_lda.fit(X_train, y_train)\n",
    "print('Accuracy score', sklearn_lda.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\"> Intermezzo - Hyperparameter optimization </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"intermezzo\"></a> \n",
    "\n",
    "Most models, including support vector machines that will be covered in the next sections, have tunable hyperparameters. Hyperparameters are parameters whose values control the learning process and determine the values of model parameters that a learning algorithm ends up learning. In the case of the SVM the main hyperparameter is the regularization parameter (the lambda added before the norm in the regularization norm).  But the kernel itself can also be supplied as a hyperparameter. \n",
    "\n",
    "Two searches are often proposed to tune a machine learning model in function of its hyperparameters: \n",
    "- the first considers an exhaustive and structured search in the full hyperparameter space; \n",
    "- the second applies random combinations of hyperparameter settings. \n",
    "\n",
    "![grid](./grid_vs_randomgrid.png)\n",
    "\n",
    "[It has been shown that when only few hyperparameters have an impact on model performance a randomized grid search is preferred. ](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) \n",
    "\n",
    "`Scikit-learn` offers implementations of [both](http://scikit-learn.org/stable/modules/grid_search.html) within a cross-validation scheme for hyperparameter tuning (see slide 36 of Lecture 2a on nested cross-validation).\n",
    "\n",
    "![CV](./Slide36.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\"> Support vector machines (SVM) </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_2\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Exercise: \n",
    "<ol>\n",
    "<li> Choose one of the datasets down below to continue the exercise with.\n",
    "<li> Perform an 80/20 train-test split.\n",
    "<li> Define a hyperparameter dictionary with multiple values of the 'C' parameter in the SVC model and a linear and rbf kernel (<b>Hint:</b> look at the documentation of the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">GridSearchCV</a> or <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\">RandomSearchCV</a> to  find the exact syntax).\n",
    "<li> Define the model (without the hyperparameters specified in the dictionary).\n",
    "<li> Implement the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">GridSearchCV</a> or the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\">RandomSearchCV</a> (or both 😉), fit the constructor and make predictions.\n",
    "<li> Look at the optimal hyperparameters selected and evaluate your predictions with <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\">the accuracy score metric</a>. \n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.750878</td>\n",
       "      <td>0.321070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.144829</td>\n",
       "      <td>0.990214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.765681</td>\n",
       "      <td>0.028995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.658622</td>\n",
       "      <td>-0.652663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.818977</td>\n",
       "      <td>0.612161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0 -0.750878  0.321070\n",
       "1 -0.144829  0.990214\n",
       "2 -0.765681  0.028995\n",
       "3 -0.658622 -0.652663\n",
       "4 -0.818977  0.612161"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset 1\n",
    "from sklearn.datasets import make_circles\n",
    "X_, y_= make_circles(n_samples=500, noise=0.06, random_state=42)\n",
    "X_circle, y_circle = pd.DataFrame(X_), pd.DataFrame(y_)\n",
    "X_circle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  0\n",
       "2  1\n",
       "3  0\n",
       "4  0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_circle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(x1=X_[:, 0], x2=X_[:, 1], y=y_))\n",
    "colors = {0:'blue', 1:'yellow'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('y')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x1', y='x2', label=key, color = colors[key])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the circle dataset with RandomizedSearchCV 0.91\n",
      "0.7409799098968506\n",
      "{'kernel': 'rbf', 'C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "Starttime = time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_circle, y_circle, test_size=0.20, random_state=0)\n",
    "parameters = {\"C\": np.logspace(-3,3,7), \"kernel\": ['linear', 'rbf']}\n",
    "svc_rs = RandomizedSearchCV(SVC(), parameters)\n",
    "svc_rs.fit(X_train, y_train.values.ravel())\n",
    "pred = svc_rs.predict(X_test)\n",
    "acc_rs = accuracy_score(y_test,pred)\n",
    "Endtime = time()\n",
    "\n",
    "print('Accuracy score on the circle dataset with RandomizedSearchCV', acc_rs)\n",
    "print(Endtime-Starttime)\n",
    "print(svc_rs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the circle dataset with GridSearchCV 0.91\n",
      "0.774512767791748\n",
      "{'kernel': 'rbf', 'C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "Starttime = time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_circle, y_circle, test_size=0.20, random_state=0)\n",
    "parameters = {\"C\": np.logspace(-3,3,7), \"kernel\": ['linear', 'rbf']}\n",
    "svc_gs = GridSearchCV(SVC(), parameters)\n",
    "svc_gs.fit(X_train, y_train.values.ravel())\n",
    "pred = svc_gs.predict(X_test)\n",
    "acc_gs = accuracy_score(y_test,pred)\n",
    "Endtime = time()\n",
    "\n",
    "print('Accuracy score on the circle dataset with GridSearchCV', acc_rs)\n",
    "print(Endtime-Starttime)\n",
    "print(svc_rs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ```cv_results_```will give a lot more information on the GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset 2\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X_cancer, y_cancer= load_breast_cancer(return_X_y = True, as_frame = True)\n",
    "X_cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_cancer) # the unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the circle dataset with RandomizedSearchCV 0.956140350877193\n",
      "45.02819490432739\n",
      "{'kernel': 'linear', 'C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "Starttime = time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, test_size=0.20, random_state=0)\n",
    "parameters = {\"C\": np.logspace(-3,3,7), \"kernel\": ['linear', 'rbf']}\n",
    "svc_rs = RandomizedSearchCV(SVC(), parameters)\n",
    "svc_rs.fit(X_train, y_train.values.ravel())\n",
    "pred = svc_rs.predict(X_test)\n",
    "acc_rs = accuracy_score(y_test,pred)\n",
    "Endtime = time()\n",
    "\n",
    "print('Accuracy score on the circle dataset with RandomizedSearchCV', acc_rs)\n",
    "print(Endtime-Starttime)\n",
    "print(svc_rs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the circle dataset with GridSearchCV 0.956140350877193\n",
      "80.29528427124023\n",
      "{'kernel': 'linear', 'C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "Starttime = time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, test_size=0.20, random_state=0)\n",
    "parameters = {\"C\": np.logspace(-3,3,7), \"kernel\": ['linear', 'rbf']}\n",
    "svc_gs = GridSearchCV(SVC(), parameters)\n",
    "svc_gs.fit(X_train, y_train.values.ravel())\n",
    "pred = svc_gs.predict(X_test)\n",
    "acc_gs = accuracy_score(y_test,pred)\n",
    "Endtime = time()\n",
    "\n",
    "print('Accuracy score on the circle dataset with GridSearchCV', acc_rs)\n",
    "print(Endtime-Starttime)\n",
    "print(svc_rs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\"> Optional: LDA from scratch </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"Optional\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the binary classification setting, we want to predict the probability that an instance belongs to one of two classes, given the data: $p(y|\\mathbf{x})$, also called the class posterior. In the previous lab, we saw that logistic regression directly models this probability. Classifiers that directly model the class posterior are called **discriminative classifiers**. Logistic regression is one of the most popular discriminative classification algorithms. \n",
    "\n",
    "In contrast, **generative classifiers** first model the probability of the data for each specific class $p(\\mathbf{x}|y)$ and then use Bayes' rule to invert that probability to obtain the class posterior:\n",
    "\n",
    "\\begin{equation}\n",
    "Pr(y=k|\\mathbf{x}) = \\frac{Pr(\\mathbf{x}|y=k)Pr(y=k)}{Pr(\\mathbf{x})} = \\frac{Pr(\\mathbf{x}|y=k)Pr(y=k)}{\\sum_{l=1}^{K}Pr(\\mathbf{x}|y=l)Pr(y=l)}\n",
    "\\end{equation}\n",
    "\n",
    "These models are called generative because they specify the class-conditional densities $p(\\mathbf{x}|y)$, which allows to generate new observations $\\mathbf{x}$ for a class $y$: for a given class $y$, we can sample from the distribution $p(\\mathbf{x}|y)$. \n",
    "\n",
    "**LDA is an example of a generative classifier.** LDA assumes that the data in each class can be modelled as a multivariate Gaussian distribution. Furthermore, the assumption is made that the (co)variance within and between predictor variables is the same for all the classes. As such, only a limited number of parameters have to be estimated from the data to fully specify the class-conditional distributions: in the multivariate case, we need to estimate the class-specific mean vectors $\\mathbf{\\mu_{k}}$ and the common covariance matrix $\\Sigma$. In the case of just one predictor, this simplifies to estimating the mean of $x$ within each class and the variance $\\sigma^2$. The priors $p(y=k)$ are simply estimated from the data by calculating the proportion of training instances in each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To introduce the LDA, we will make use of a simple 1-dimensional toy dataset that contains two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.418646</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.485199</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.983165</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.732385</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.291976</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x    y\n",
       "0  2.418646  1.0\n",
       "1  0.485199  1.0\n",
       "2  0.983165  1.0\n",
       "3  3.732385  1.0\n",
       "4  0.291976  1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data\n",
    "data = pd.read_csv('toydata_LDA.csv')\n",
    "type(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x153244a9a140>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split in train/test sets following a 90/10 ratio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['x'], data['y'], test_size=0.10, random_state=0)\n",
    "\n",
    "# The code down below will visualize the data and the train-test split\n",
    "# (The y-axis contains just random scatter as the toy data is 1D)\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.scatter(X_train, np.random.normal(scale=0.001,size=len(X_train)),\n",
    "           color=['#b2182b' if y else '#2166ac' for y in y_train],\n",
    "          edgecolor='black')\n",
    "ax.get_yaxis().set_ticks([]);\n",
    "ax.scatter(X_test, np.random.normal(scale=0.001, size=len(X_test)),\n",
    "          color='white', edgecolor=['#b2182b' if y else '#2166ac' for y in y_test], linewidth=3, s=50)\n",
    "ax.legend(['Training data', 'Test data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to classify the test data in white with LDA. In order to do so, we will need to estimate the prior probabilities of an observation belonging to each class and the class-conditional densities. In LDA, we make a convenient assumption: we model the class-conditional densities with a normal distribution. In that way, we only need to estimate the mean and the variance for each class. Furthermore, we assume that the variance is the same in both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3> Exercise </h3>  \n",
    "<p>\n",
    "Using the training data, estimate the parameters that are required for LDA. The priors $p(y=k)$ are simply estimated from the data by calculating the proportion of training instances in each class.To estimate the variance, use the following formula (see slide 6 of the theory lectures), where $K$ denotes the number of classes: \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "![formula](./Slide6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_0: 0.273015873015873\n",
      "p_1: 0.726984126984127\n",
      "mu_0: -1.197017433094278\n",
      "mu_1: 1.524863521496\n",
      "sigma_sq: 1.473922864862082\n"
     ]
    }
   ],
   "source": [
    "# estimate the prior probabilities\n",
    "p_0 = len(X_train[y_train == 0])/len(y_train)\n",
    "p_1 = len(X_train[y_train == 1])/len(y_train)\n",
    "\n",
    "# Estimate the mean of both gaussians\n",
    "mu_0 = np.mean(X_train[y_train == 0])\n",
    "mu_1 = np.mean(X_train[y_train == 1])\n",
    "\n",
    "# Estimate the variance sigma²\n",
    "sigma_sq = ((np.sum((X_train[y_train == 0] - mu_0)**2))+(np.sum((X_train[y_train == 1] - mu_1)**2)))/(len(y_train)- 2)\n",
    "\n",
    "print('p_0: ' + str(p_0))\n",
    "print('p_1: ' + str(p_1))\n",
    "print('mu_0: ' + str(mu_0))\n",
    "print('mu_1: ' + str(mu_1))\n",
    "print('sigma_sq: ' + str(sigma_sq)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Exercise</h3>\n",
    "<p>\n",
    "Now, for each instance $x$ in the test data set, use the parameters above to fill in Bayes' rule and to calculate the probability of that instance to belong to each class. Use scipy.stats.norm.pdf(x, location, scale) to calculate the pdf of the normal distribution. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 94.29%\n"
     ]
    }
   ],
   "source": [
    "import scipy as sc \n",
    "\n",
    "prob_class_0 = []\n",
    "prob_class_1 = []\n",
    "\n",
    "for x in X_test:\n",
    "    likelihood_0 = sc.stats.norm.pdf(x, loc=mu_0, scale=sigma_sq**0.5)\n",
    "    likelihood_1 = sc.stats.norm.pdf(x, loc=mu_1, scale=sigma_sq**0.5)\n",
    "    marginal = p_0*likelihood_0 + p_1*likelihood_1\n",
    "    \n",
    "    posterior_0 = p_0*likelihood_0/marginal\n",
    "    posterior_1 = p_1*likelihood_1/marginal\n",
    "    prob_class_0.append(posterior_0)\n",
    "    prob_class_1.append(posterior_1)\n",
    "\n",
    "# ** solution **\n",
    "predicted_classes = [0 if prob_class_0[i] > prob_class_1[i] else 1 for i in range(len(prob_class_0))]\n",
    "\n",
    "accuracy = np.round(np.mean(predicted_classes == y_test)*100,2)\n",
    "print('Classification accuracy: {}%'.format(accuracy))\n",
    "\n",
    "# Visualize the predictions\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.scatter(X_train, np.random.normal(scale=0.001,size=len(X_train)),\n",
    "           color=['#b2182b' if y else '#2166ac' for y in y_train],\n",
    "          edgecolor='black')\n",
    "ax.get_yaxis().set_ticks([]);\n",
    "test_jitter = np.random.normal(scale=0.001, size=len(X_test))\n",
    "ax.scatter(X_test, test_jitter,\n",
    "          color='white', edgecolor=['#b2182b' if y else '#2166ac' for y in y_test], s=100, linewidth=3)\n",
    "ax.scatter(X_test, test_jitter,\n",
    "          color=['#b2182b' if y else '#2166ac' for y in predicted_classes], s=20)\n",
    "ax.set_title('Toy dataset - LDA predictions. Accuracy: {}'.format(accuracy)).set_fontsize(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "        <p>\n",
    "The two classes from the toy dataset were simulated from two Gaussian distributions with mean -1.5 for class 0 and 1.5 for class 1, and a standard deviation of 1.2. Run the code below to see how well LDA has modelled these distributions. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    xrange = np.arange(-5,5,0.01)\n",
    "    dist_0 = sc.stats.norm.pdf(xrange, loc=-1.5, scale=1.2)\n",
    "    dist_1 = sc.stats.norm.pdf(xrange, loc=1.5, scale=1.2)\n",
    "    est_0 = sc.stats.norm.pdf(xrange, loc=mu_0, scale=sigma_sq**0.5)\n",
    "    est_1 = sc.stats.norm.pdf(xrange, loc=mu_1, scale=sigma_sq**0.5)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    ax.scatter(xrange, dist_0, color='#2166ac', marker='.')\n",
    "    ax.scatter(xrange, dist_1, color='#b2182b', marker='.')\n",
    "    ax.scatter(xrange, est_0, color='#92c5de', marker='.')\n",
    "    ax.scatter(xrange, est_1, color='#d6604d', marker='.')\n",
    "\n",
    "    # Add the training points \n",
    "    ax.scatter(X_train, np.random.normal(loc=-0.1,scale=0.005,size=len(X_train)),\n",
    "           color=['#b2182b' if y else '#2166ac' for y in y_train],\n",
    "          edgecolor='black')\n",
    "    ax.get_yaxis().set_ticks([]);\n",
    "    ax.legend(['True distribution of class 0',\n",
    "            'True distribution of class 1', \n",
    "            'Estimated pdf of class 0',\n",
    "            'Estimated pdf of class 1'],\n",
    "             fontsize=14)\n",
    "except:\n",
    "    print('Estimate the parameters of the Gaussians first: mu_0, mu_1, sigma_sq')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5ad70b114478dbfeb7686dda59c481057deed804443829ded50851cf484fa5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
