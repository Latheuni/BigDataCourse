{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "tf.random.set_seed(42) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Practical session 10: Deep Learning\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2>Table of Contents </h2><a class=\"anchor\" id=\"top\"></a>\n",
    "    <br><a href=\"#section_1\">1. Data loading</a>\n",
    "    <br><a href=\"#section_2\">2. Preprocessing the dataset</a>\n",
    "    <br><a href=\"#section_3\">3. Implementing a neural network</a>\n",
    "    <br>&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#section_3_1\">3.1 Define the architecture</a>\n",
    "    <br>&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#section_3_2\">3.2 Define the loss function, optimizer, and metrics</a>\n",
    "    <br>&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#section_3_3\">3.3 Run the learning procedure to train the network</a>\n",
    "    <br>&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#section_3_4\">3.4 Inspect classifier performance</a>\n",
    "    <br><a href=\"#section_4\">Extra: A look at intermediate representations of the network</a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Malaria cell classification\n",
    "\n",
    "Malaria is a blood disease caused by the *Plasmodium* parasite, and is transmitted through the bite of the female *Anopheles* mosquito. The disease is mostly diagnosed by counting parasitized blood cells in a blood smear under a microscope. However, manual cell counting is an exhausting, error-prone procedure. This can negatively affect the quality of the diagnosis [[1]](https://peerj.com/articles/4568/). Especially in *resource-constrained* regions of the world, difficult working conditions lead to poor diagnosis quality [[2]](https://lhncbc.nlm.nih.gov/publication/pub9932) .\n",
    "\n",
    "In this practical session we will develop a deep learning pipeline that will aid in improving malaria diagnosis by automating infected cell counting. To this end we will use the Malaria cell dataset [[2]](https://lhncbc.nlm.nih.gov/publication/pub9932) to train a neural network that predicts a cell's infection state based on a microscopy image of it. The microscopy images were acquired using a smartphone attached to a small portable microscope.\n",
    "\n",
    "To set up the deep learning pipeline we will go through these steps:\n",
    "- Load the dataset\n",
    "-  Preprocess the dataset\n",
    "-  Define a neural network\n",
    "-  Define the learning procedure\n",
    "-  Train the neural network\n",
    "- Inspect the network's performance\n",
    "\n",
    "We will use the [TensorFlow machine learning framework](http://tensorflow.org) to implement these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\">1. Data loading</h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_1\"></a> \n",
    "\n",
    "### Device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def select_device(prefer_gpu=True):\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "    if (len(gpus) > 0) and prefer_gpu:\n",
    "        return gpus[0]\n",
    "    else:\n",
    "        return [x.name for x in local_device_protos if x.device_type == 'CPU'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code sets the device to use to GPU if you have one available\n",
    "device = select_device(prefer_gpu=True)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above `device` is used in the notebook to select where code is executed. This is done with a `with` statement:\n",
    "\n",
    "```\n",
    "with tf.device(device):\n",
    "    # code\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Malaria dataset\n",
    "\n",
    "We use the `tensorflow-datasets` package to load the malaria dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = tfds.builder('malaria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes around 10 minutes\n",
    "builder.download_and_prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some info about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "builder.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the builder to extract a TensorFlow Dataset for efficient access to the images.\n",
    "We also split the dataset into training data for training the neural network parameters, and testing data for evaluating the network's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    train_ds, test_ds = (\n",
    "        builder.as_dataset(as_supervised=True, split=\"train[:10%]\"),\n",
    "        builder.as_dataset(as_supervised=True, split=\"train[-90%:]\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3> Exercise</h3>\n",
    "    \n",
    "Plot some images from `train_ds` to get an idea of what the data looks like. Have a look at the [TensorFlow Datasets documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to see how you can *take* a number of images from the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_images = 10\n",
    "\n",
    "# select some images from the dataset\n",
    "images = ...\n",
    "\n",
    "fig, axes = plt.subplots(1, n_images, figsize=(20, 5), dpi=100)\n",
    "for ax, (image, label) in zip(axes, images):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(\"Healthy\" if label.numpy() == 1 else \"Malaria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\">2. Preprocessing the dataset</h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_2\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset two preprocessing operations are required:\n",
    "1. Resize all images to uniform width and height, and\n",
    "2. normalize the pixel value range to [0,1].\n",
    "\n",
    "Neural networks have a fixed architecture and can therefore only take inputs of equal dimensions. This is why we have to preprocess the images to have uniform dimensions. For the Malaria dataset, we downsample the images to be 40 pixels high and 40 pixels wide. We will need the `resize_with_pad` and `resize` functions from the `tf.image` module.\n",
    "\n",
    "Normalization of the pixel range to a [0,1] range is done to improve the stability of the weight updates. The Malaria dataset contains 8-bit RGB images, which have a maximum pixel value of 255 and a minimum of 0. If we would train the neural network using pixel values in the [0,255] range, the network's weights could grow too large causing high values during backpropagation and unstable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the first preprocessing function\n",
    "# note: in this function we also cast the images to the float32 data type.\n",
    "\n",
    "image_width, image_height = 40, 40\n",
    "def resize_images(image, label):\n",
    "    return (\n",
    "        tf.cast(tf.image.resize_with_pad(tf.image.resize(image, (image_width, image_height)), image_width, image_height), tf.float32), \n",
    "        tf.cast(label, tf.float32)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the second preprocessing function\n",
    "def minmax_normalization(image, label):\n",
    "    return (\n",
    "        image / tf.math.reduce_max(tf.reshape(image, [-1, image.shape[-1]]), axis=0), # divide each pixel in the image by the maximum value in each channel (R, G, and B)\n",
    "        label\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3> Exercise</h3>\n",
    "    \n",
    "Apply both preprocessing functions to the train and test dataset defined earlier. Have a look at the [TensorFlow Datasets documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to see how you can *map* a function over the elements of a dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply resize images\n",
    "train_ds = ...\n",
    "test_ds = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply minmax\n",
    "train_ds = ...\n",
    "test_ds = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# have a look at the preprocessed images...\n",
    "\n",
    "n_images = 10\n",
    "fig, axes = plt.subplots(1, n_images, figsize=(20, 5), dpi=100, sharex=True, sharey=True)\n",
    "for ax, (image, label) in zip(axes, train_ds.take(n_images)):\n",
    "    ax.imshow(image.numpy())\n",
    "    ax.set_title(\"Healthy\" if label.numpy() == 1 else \"Malaria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now preprocessed and ready to handled by the neural network that we will define in the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\">3. Implementing a neural network</h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_3\"></a> \n",
    "\n",
    "We need a neural network that is capable of classifying cell images in a positive and negative class. In this case, we want it to distinguishing between a healthy and parasitized cell based on a 40x40 RGB image. For today's standards this specific problem is a fairly easy computer vision task. A simple (and efficient) _shallow_ convolutional neural network (CNN) will suffice.\n",
    "\n",
    "<br>\n",
    "\n",
    "<h3 style=\"display: inline\">3.1 Define the architecture</h3> <a class=\"anchor\" id=\"section_3_1\"></a>\n",
    "\n",
    "The neural network will consist out of 4 main layers: 2 feature extraction layers, and 2 classification layers.\n",
    "\n",
    "The **feature extraction** layers are made up of [convolution](https://developers.google.com/static/machine-learning/practica/image-classification/images/convolution_overview.gif) and [max-pooling operations](https://developers.google.com/static/machine-learning/practica/image-classification/images/maxpool_animation.gif) and activation functions. They will learn to extract relevant features from the image and enable the classification layers to learn to classify cells into the healthy or parasitized category.\n",
    "\n",
    "The extracted features get passed on to the **classification layers**, which are made up of densely connected layers, dropout connections and activation functions.\n",
    "\n",
    "The [**activation functions**](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*XxxiA0jJvPrHEJHD4z893g.png) used in this neural network are the sigmoid, and Rectified Linear Unit (ReLU). The ReLU is used as the intermediate activation function in the feature extraction and classification layers. This function is used in many state-of-the-art image classification networks. It works well because it prevents the gradients from *vanishing* during backpropagation. The sigmoid is the activation function applied to the output of the final densely connected layer. It squeezes whatever value that comes out of the network to the 0-1 range. Ideal for binary classification!\n",
    "\n",
    "![SegmentLocal](network_diagram.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3> Exercise</h3>\n",
    "    \n",
    "Define a CNN architecture with the [TensorFlow Keras API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) using the above scheme as a guideline. Feel free to try out some of your own settings! The network takes RGB images as input, and has one output neuron that will produce output values between 0 (malaria) and 1 (healthy).\n",
    "    \n",
    "You will need the following layers:\n",
    "    <ul>\n",
    "    <li> `tf.keras.layers.Conv2D`</li>\n",
    "    <li> `tf.keras.layers.MaxPool2D`</li>\n",
    "    <li>`tf.keras.layers.ReLU`</li>\n",
    "    <li>`tf.keras.layers.Flatten`</li>\n",
    "    <li>`tf.keras.layers.Dense`</li>\n",
    "    <li>`tf.keras.layers.Dropout`</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    classifier = tf.keras.Sequential([\n",
    "        ...\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"display: inline\">3.2 Define the loss function, optimizer, and metrics</h3> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_3_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the parameters of the network we need an optimization algorithm, and a loss function to minimize. In this particular case the binary cross-entropy is a good choice.\n",
    "\n",
    "$$\\textrm{BinaryCrossEntropy} = -\\frac{1}{N}\\sum_{i=1}^N{(y_i\\cdot\\log(p_i) + (1 - y_i)\\cdot\\log(1 - p_i))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bce_loss = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As optimization algorithm we choose stochastic gradient descent (SGD). More advanced optimizers exist, but SGD is a good first choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary cross-entropy is a good metric for optimization, but is less interpretable. Accuracy is a more intuitive metric for assessing model performance.\n",
    "$$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_metric = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, TensorFlow requires us to `compile` all parts of the learning procedure together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"display: inline\">3.3 Run the learning procedure to train the network</h3> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_3_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define some constants for the learning procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 30 # How many times will the network see all training data\n",
    "batch_size = 256 # How many instances will the network process in one iteration\n",
    "\n",
    "train_len = int(builder.info.splits[\"train\"].num_examples*0.6)\n",
    "test_len = int(builder.info.splits[\"train\"].num_examples*0.4)\n",
    "steps_per_epoch = train_len//batch_size\n",
    "test_steps = test_len//batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we iteratively update the parameters of the network using mini-batches of data (of size `batch_size`). In other words, we fit the network to the data. In pseudo-code:\n",
    "```\n",
    "for epoch in epochs:\n",
    "    for true_labels, batch in batches:\n",
    "        predictions = network(batch)\n",
    "        loss = loss_function(true_labels, predictions)\n",
    "        optimizer.update(network, loss)\n",
    "```\n",
    "The TensorFlow Keras API has several utitility functions that wrap basic procedures in helper functions. One such function is `fit` ([documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit)). It performs something close to the above pseudo-code on our network. All we have to do, is pass it the constants we defined above. Using helper functions is good practice as it lets us write code quicker, and it is less error-prone. However, always know what the helper functions do exactly!\n",
    "\n",
    "Note that we save the return value of the fit function to a variable. It contains important information about the performance of our network. We will analyze this in the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    history = classifier.fit(\n",
    "        train_ds.batch(batch_size).repeat(), # on which data to we want to train\n",
    "        epochs=epochs, # how many epochs do we want to run\n",
    "        steps_per_epoch=steps_per_epoch, # how many steps are in one epoch\n",
    "        validation_data=test_ds.batch(batch_size).repeat(), # what test data do we want to use\n",
    "        validation_steps=test_steps # how many steps do we need to take when testing\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot binary_accury and val_binary_accuracy\n",
    "\n",
    "plt.plot(history.epoch, history.history[\"binary_accuracy\"], color=\"red\")\n",
    "plt.plot(history.epoch, history.history[\"val_binary_accuracy\"], color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot loss and val_loss\n",
    "\n",
    "plt.plot(history.epoch, history.history[\"loss\"], color=\"red\")\n",
    "plt.plot(history.epoch, history.history[\"val_loss\"], color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using a dropout layer to improve generalization. This layer randomly shuts of connections during training. During testing, this layer is disabled and all connections are always enabled. This explains why the testing accuracy is often higher then the training accuracy, and the testing loss is often lower then the training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Exercise</h3>\n",
    "\n",
    "Many other optimizers exist besides the SGD optimizer that we used. For example, the Adam optimizer combines two extensions of SGD into one powerful optimizer. Try to use the Adam optimizer to fit the neural network. See how the training progresses - do you notice any difference compared to SGD?\n",
    "    \n",
    "<b>Note:</b> In order to retrain the classifier with another optimizer you have to reinstantiate the classifier.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    history = classifier.fit(\n",
    "        train_ds.batch(batch_size).repeat(), # on which data to we want to train\n",
    "        epochs=epochs, # how many epochs do we want to run\n",
    "        steps_per_epoch=steps_per_epoch, # how many steps are in one epoch\n",
    "        validation_data=test_ds.batch(batch_size).repeat(), # what test data do we want to use\n",
    "        validation_steps=test_steps # how many steps do we need to take when testing\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"display: inline\">3.4 Inspect classifier performance</h3> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_3_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the network, we are interested in its eventual performance and how this evolved during the fitting procedure. By comparing metrics computed on the training and test dataset we can spot overfitting as well: if the training accuracy is much higher compared to testing, the network is overfitting.\n",
    "\n",
    "The information we need, is returned by the fit function in a [`History`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History) object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Exercise</h3>\n",
    "\n",
    "Plot the the accuracy and loss in function of the epochs for the train and validation set. Use the `history` object returned by the `fit` function.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot binary_accury and val_binary_accuracy\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot loss and val_loss\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Exercise</h3>\n",
    "\n",
    "Use the trained network to compute prediction scores for the test dataset. Use these scores to compute the confusion matrix and ROC AUC score.\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "true = [l.numpy() for i,l in test_ds.take(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute confusion matrix\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute roc auc score\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\">Extra: A look at intermediate representations of the network</h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_4\"></a> \n",
    "\n",
    "As an image passes through our convolutional neural network, it is transformed by the operations defined in the layers. Each transformation extracts the most relevant information from the input it receives, and passes it on to the next layer. The output of each intermediate layer is called a feature map or *representation*.\n",
    "\n",
    "By recording these intermediate representations for an image, we get an idea of what the network is focusing on to make its prediction. This way, we get some insight into the so-called *black-box model*.\n",
    "\n",
    "In this last part we will ook at some of the representations learned by the network we trained. Before running the code, think about what these representations might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract some images to analyse\n",
    "n_images = 10\n",
    "it = iter(test_ds.batch(n_images))\n",
    "images, labels = next(it)\n",
    "\n",
    "# get the predictions\n",
    "predictions = classifier.predict(images)\n",
    "\n",
    "# record the intermediate representations\n",
    "representations = []\n",
    "record = [1, 2, 3]\n",
    "x = images\n",
    "for i, layer in enumerate(classifier.layers):\n",
    "    x = layer(x)\n",
    "    if i in record:\n",
    "        representations.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1+len(representations), len(images), figsize=(40, 10), constrained_layout=True)\n",
    "    \n",
    "for ax,img,label,prediction in zip(axes[0, :], images, labels, predictions):\n",
    "    ax.imshow(img)\n",
    "    txt_label = \"Healthy\" if label.numpy() == 1 else \"Malaria\"\n",
    "    txt_pred = \"Healthy\" if prediction > 0.5 else \"Malaria\"\n",
    "    ax.set_title(f\"{txt_label}, ({txt_pred})\")\n",
    "    \n",
    "for reps, row in zip(representations, axes[1:, :]):\n",
    "    for rep, ax in zip(reps, row):\n",
    "        ax.imshow(rep[..., 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:BigDataScience]",
   "language": "python",
   "name": "conda-env-BigDataScience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
