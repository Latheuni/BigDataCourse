{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OJnMyT3xMkVW"
      },
      "source": [
        "# Practical session: Representation learning - Variational autoencoders"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For those of you who are opening this in google colab. To speed all the computations in this notebook up, you can access gpu's from Google. In the upper right corner of google colab there is a small icon that says RAM and Disk (if this is not the case, select the arrow pointing downwards and select connect to a hosted runtime) . If you click there a subtab will open that says on the bottom *change runtime type*. Click on this and select GPU's under hardware accellerator and save your changes. \n",
        "\n",
        "Doing this will delete your variables (if there are any in the current session). So you will need to rerun all cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jrOOdI7F1aS",
        "outputId": "b1095174-a711-4d0b-c1e1-d923cf315dd9"
      },
      "outputs": [],
      "source": [
        "%pip install tensorflow-probability\n",
        "\n",
        "# to generate gifs\n",
        "%pip install imageio\n",
        "%pip install git+https://github.com/tensorflow/docs\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7MlpTy8IRs9"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBJ3DffwISuN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def select_device(prefer_gpu=True):\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
        "    if (len(gpus) > 0) and prefer_gpu:\n",
        "        return gpus[0]\n",
        "    else:\n",
        "        return [x.name for x in local_device_protos if x.device_type == 'CPU'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_Nt8FU4BIVRJ",
        "outputId": "8192e6db-70a7-4661-952c-ed8ab1c9380b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This code sets the device to use to GPU if you have one available\n",
        "device = select_device(prefer_gpu=True)\n",
        "device"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    <h2>Table of Contents </h2><a class=\"anchor\" id=\"top\"></a>\n",
        "    <br><a href=\"#section_1\">Theory recap</a>\n",
        "    <br><a href=\"#section_2\">Practical session</a>\n",
        "    <br><a href=\"#section_3\">Preprocessing the data</a>\n",
        "    <br><a href=\"#section_4\">Defining the variational autoencoder architecture</a>\n",
        "    <br><a href=\"#section_5\">Training the model and generating new images</a>\n",
        "</div>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8EZ4sf5hG5Mt"
      },
      "source": [
        "<h2 style=\"display: inline\"> Theory recap </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_1\"></a> \n",
        "\n",
        "As seen in the theory lecture on Representation learning, the end goal of training a variational auto-encoder is finding a model that approximates the data generating distribution $p_{data}$:\n",
        "\n",
        "$$\\theta*=\\textrm{arg}\\textrm{min}_{\\theta\\in\\mathcal{M}}d(p_{data},p_\\theta)$$\n",
        "\n",
        "with $d$ the Kullback-Leibler divergence.\n",
        "\n",
        "It can be shown that this minimization is equivalent to maximizing the log-likelihood $log{p_\\theta}$ over a discrete dataset sampled from $p_{data}$. However, computing $log{p_\\theta}$ is intractable due to the marginalization over $Z$, the latent random variable.\n",
        "\n",
        "Therefore, we approximate this problem by maximizing a (tractable) lower-bound on $log{p_\\theta}$, namely, the Evidence Lower BOund (ELBO):\n",
        "\n",
        "$$\\log p_\\theta(x) \\geq \\mathbb{E}_{z\\sim q_\\lambda(z|x)}[\\log{\\frac{p_\\theta(x|z)}{q_\\lambda(z|x)}}]$$\n",
        "\n",
        "In order to be able to sample meaningful latent representations, we impose a unit Gaussian prior on $Z$. This is often reffered to as teh reparameterization trick. Without the reparameterization trick, backpropagation would be stuck at the random bottleneck node. We cannot backpropagate through a random variable.\n",
        "\n",
        "Therefore, we introduce a normally distributed random variable $\\epsilon$ and we let the encoder output the parameters of a multivariate normal distribution as a vector. A latent representation for an input $x$ can then be constructed as follows:\n",
        "1. Sample from $\\epsilon\\sim\\mathcal{N}(O, \\mathbb{1})$\n",
        "2. $z = \\mu + \\sigma \\odot \\epsilon$\n",
        "\n",
        "This $z$ can then be passed on to the decoder. The backpropagation can now flow through the parameter vector, that is output by the encoder.\n",
        "\n",
        "This tricks allows us to construct the VAE loss function:\n",
        "\n",
        "$$\\mathcal{L}(x; \\theta, \\lambda) = \\mathbb{E}_{z\\sim q_\\lambda(z|x)}[\\log{p_\\theta(x|z)}] - d_{KL}(q_\\lambda(z|x), p_\\theta(z))$$\n",
        "\n",
        "By weighting the $d_{KL}$-term, we obtain the $\\beta$-VAE loss function:\n",
        "\n",
        "$$\\mathcal{L}(x; \\theta, \\lambda, \\beta) = \\mathbb{E}_{z\\sim q_\\lambda(z|x)}[\\log{p_\\theta(x|z)}] - \\beta*d_{KL}(q_\\lambda(z|x), p_\\theta(z))$$\n",
        "\n",
        "Here, we approximate $\\mathcal{L}(x; \\theta, \\lambda, \\beta)$ using a single-sample Monte Carlo Estimate:\n",
        "\n",
        "$$\\log{p_\\theta(x|z)} + \\beta*(\\log{p_\\theta(z)} - \\log{q_\\lambda(z|x)})$$\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kui2LQciHmW2"
      },
      "source": [
        "<h2 style=\"display: inline\"> Practical session</h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_2\"></a> \n",
        "\n",
        "Today we will construct a variational autoencoder on the MNIST dataset, with the goal to generate new digit pictures. Each MNIST image is originally a vector of 784 integers, each of which is between 0-255 and represents the intensity of a pixel.\n",
        "\n",
        "Since we are dealing with a computer vision task, we will use a **convolutional** variational autoencoder (CVAE). The CVAE has the same bottleneck structure as a standard variational autoencoder, but replaces the dense layers with convolutional and tranposed convolutional layers. The former type used in the encoder for down-sampling, and the latter in the decoder for upsampling. As with any autoencoder, the goal of the CVAE is to learn the *identity function*: learn to reconstruct the input image after \"squeezing\" it through a low-dimensional latent representation, also called the *bottleneck*.\n",
        "\n",
        "Additionally, we will use a **variational** autoencoder for this task. This means we will use variational inference to find the optimal $\\theta$ and $\\lambda$ to parameterize, respectively, the decoder $p_\\theta(x|z)$ and the encoder $q_\\lambda(z|x)$, with $z$ the latent representation.\n",
        "\n",
        "In order for us to be able to use $p_\\theta(x|z)$, we need to first be able to choose a meaningfull $z$. Therefore, we will impose a Gaussian prior over $q_\\lambda(z|x)$. We'll see how this is done when we define the loss\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zci_78osJL2A"
      },
      "source": [
        "<h2 style=\"display: inline\"> Preprocessing the data</h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_2\"></a> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm2g08uUF3dS"
      },
      "outputs": [],
      "source": [
        "# Load in the data\n",
        "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize the pixel value range to [0.1] and threshold all pixels at 0.5\n",
        "def preprocess_images(images):\n",
        "  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
        "  return np.where(images > .5, 1.0, 0.0).astype('float32')\n",
        "\n",
        "# Perform training and test splits\n",
        "train_images = preprocess_images(train_images)\n",
        "test_images = preprocess_images(test_images)\n",
        "\n",
        "# Set parameters\n",
        "train_size = 60000\n",
        "batch_size = 256\n",
        "test_size = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4b2KX9CGCHu"
      },
      "outputs": [],
      "source": [
        "# Shuffle the dataset and divide in batches\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)\n",
        "                 .shuffle(train_size).batch(batch_size))\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\n",
        "                .shuffle(test_size).batch(batch_size))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Iu_W1-sJwTl"
      },
      "source": [
        "<h2 style=\"display: inline\"> Defining the variational autoencoder architecture</h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_4\"></a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qfgyaaJJ429"
      },
      "source": [
        "**TASK 1**\n",
        "\n",
        "Implement the reparameterization trick in the `reparameterize` function below.\n",
        "\n",
        "**NOTE**:\n",
        "We use the log-normal distribution for numerical stability during training. This means:\n",
        "\n",
        "$$ \\sigma = \\sqrt{e^\\textrm{logvar}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK6yEUlpJvgc"
      },
      "outputs": [],
      "source": [
        "class CVAE(tf.keras.Model):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(CVAE, self).__init__()\n",
        "        \n",
        "        subsampled_size = 7\n",
        "        input_shape = (28, 28, 1)\n",
        "        \n",
        "        self.latent_dim = latent_dim\n",
        "        self.inference_net = tf.keras.Sequential([\n",
        "              tf.keras.layers.InputLayer(input_shape=input_shape),\n",
        "              tf.keras.layers.Conv2D( # This layer will downsample the image to half its width and height\n",
        "                  filters=32, kernel_size=3, strides=(2,2), activation='relu'),\n",
        "              tf.keras.layers.Conv2D( # This layer will downsample the image again to half its current width and height\n",
        "                  filters=64, kernel_size=3, strides=(2,2), activation='relu'),\n",
        "              tf.keras.layers.Flatten(),\n",
        "              # No activation\n",
        "              tf.keras.layers.Dense(latent_dim + latent_dim)])\n",
        "\n",
        "        self.generative_net = tf.keras.Sequential([\n",
        "              tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "              tf.keras.layers.Dense(units=subsampled_size*subsampled_size*32, activation=tf.nn.relu),\n",
        "              tf.keras.layers.Reshape(target_shape=(subsampled_size, subsampled_size, 32)),\n",
        "              tf.keras.layers.Conv2DTranspose( # This layer will upsample the image to twice its current width and height\n",
        "                  filters=64,\n",
        "                  kernel_size=3,\n",
        "                  strides=2,\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              tf.keras.layers.Conv2DTranspose( # This layer will upsample the image again to twice its current width and height\n",
        "                  filters=32,\n",
        "                  kernel_size=3,\n",
        "                  strides=2,\n",
        "                  padding=\"SAME\",\n",
        "                  activation='relu'),\n",
        "              # No activation\n",
        "              tf.keras.layers.Conv2DTranspose(\n",
        "                  filters=input_shape[-1], kernel_size=3, strides=1, padding=\"SAME\")] )\n",
        "\n",
        "    @tf.function\n",
        "    def sample(self, eps=None):\n",
        "        if eps is None:\n",
        "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
        "        return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "    def encode(self, x):\n",
        "        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
        "        return mean, logvar\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        # sample random noise\n",
        "        ...\n",
        "        #perform reparameterization\n",
        "        ...\n",
        "        return ...\n",
        "\n",
        "    def decode(self, z, apply_sigmoid=False):\n",
        "        logits = self.generative_net(z)\n",
        "        if apply_sigmoid:\n",
        "            probs = tf.sigmoid(logits)\n",
        "            return probs\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ox0HAXXJ76E"
      },
      "source": [
        "**TASK 2**\n",
        "\n",
        "Complete the missing parts in the function below to compute the loss.\n",
        "\n",
        "Remember here, we approximate $\\mathcal{L}(x; \\theta, \\lambda, \\beta)$ using a single-sample Monte Carlo Estimate:\n",
        "\n",
        "$$\\log{p_\\theta(x|z)} + \\beta*(\\log{p_\\theta(z)} - \\log{q_\\lambda(z|x)})$$\n",
        "\n",
        "You could also analytically compute the KL term, but here we incorporate all three terms in the Monte Carlo estimator for simplicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnPsILDgKASt"
      },
      "outputs": [],
      "source": [
        "def log_normal_pdf(sample, mean, logvar, raxis=1): \n",
        "    log2pi = tf.math.log(2. * np.pi)\n",
        "    return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE7lwb_kKFMN"
      },
      "outputs": [],
      "source": [
        "def compute_loss(model, x_true, beta=1):\n",
        "    mu, logvar = ... # parameters for the multivariate normal posterior\n",
        "    z = ...  # a sample from the posterior\n",
        "    x_recons_logits = ... # the reconstruction of the sample\n",
        "    \n",
        "    raw_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=x_true,\n",
        "                        logits=x_recons_logits)\n",
        "    \n",
        "    neg_log_likelihood = tf.math.reduce_sum(raw_cross_entropy, axis=[1, 2, 3])\n",
        "    \n",
        "    logpz = ... # density of the prior evaluated at z_sample\n",
        "    logqz_x = ... # density of the latent posterior evaluated at z_sample\n",
        "    kl_divergence = logqz_x - logpz\n",
        "    \n",
        "    elbo = tf.math.reduce_mean(-beta * kl_divergence - neg_log_likelihood)\n",
        "    \n",
        "    return dict(\n",
        "        loss=-elbo, \n",
        "        reconstruction=-neg_log_likelihood, \n",
        "        kl=-kl_divergence\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz-BR40EKGBl"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def compute_apply_gradients(model, x, optimizer, beta=1):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(model, x, beta=beta)[\"loss\"]\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2 style=\"display: inline\"> Training the model and generating new images</h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_5\"></a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFuVnK0AKPF-"
      },
      "source": [
        "**TASK 3**\n",
        "\n",
        "Now it is time to train the $\\beta$-VAE.\n",
        "\n",
        "First, create a CVAE model with 2 latent dimensions. Then, create an Adam optimizer with a learning rate of 0.001. Finally, implement the iterative training procedure.\n",
        "\n",
        "Training pseudocode:\n",
        "```\n",
        "for every epoch\n",
        "    for every batch in train dataset\n",
        "        compute loss and apply gradients\n",
        "        \n",
        "    for every batch in test dataset\n",
        "        compute loss\n",
        "    \n",
        "    print test set loss\n",
        "```\n",
        "Make sure to use the `compute_apply_gradients` and `compute_loss` functions. Set $\\beta$ to 3.\n",
        "\n",
        "Complete the code to generate new images and compare the results of the first epoch to the last.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cScI39DlKPNl"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "latent_dim = 2\n",
        "\n",
        "# define model\n",
        "model = ...\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN8LFI3dPeRH"
      },
      "outputs": [],
      "source": [
        "# Pick a sample of the test set for generating output images\n",
        "num_examples_to_generate = 16\n",
        "assert batch_size >= num_examples_to_generate\n",
        "for test_batch in test_dataset.take(1):\n",
        "  test_sample = test_batch[0:num_examples_to_generate, :, :, :]\n",
        "\n",
        "def generate_images(model, test_sample):\n",
        "  mean, logvar = model.encode(test_sample)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  predictions = model.sample(z)\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i + 1)\n",
        "      plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "      plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYTfKkCuPn2W"
      },
      "outputs": [],
      "source": [
        "...  # generate images by the untrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM7IucW1KXWP"
      },
      "outputs": [],
      "source": [
        "# Note: this can take quite a while, it takes appr. 94s for one epoch without gpu's.\n",
        "for epoch ...:\n",
        "  if epoch == 2:\n",
        "    ... # generated images by the model that is trained with one e^poch\n",
        "  start_time = time.time()\n",
        "  ... # loop over the train batches\n",
        "    ... # train the model\n",
        "  end_time = time.time()\n",
        "\n",
        "  ... # loop over the test batches\n",
        "    ... # test the model, return the losses\n",
        "  elbo = ... # extract the elbo loss\n",
        "  print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\n",
        "        .format(epoch, elbo, end_time - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUP7BN17Ov9l"
      },
      "outputs": [],
      "source": [
        "generate_images(model,test_sample ) # generate images by the fully trained model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
