{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "## packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session 5: missing data, scaling and dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2>Table of Contents </h2><a class=\"anchor\" id=\"top\"></a>\n",
    "    <br><a href=\"#section_1\"> Missing data</a>\n",
    "    <br><a href=\"#intermezzo\">Intermezzo - Scaling </a>\n",
    "    <br><a href=\"#section_2\"> Dimensionality reduction </a>\n",
    "    <br><a href=\"#Optional\">  Optional - Clustering</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    " In this practical we will cover how to handle missing data, normalization and dimensionality reduction. At the end of a practical there is an optional part about cluster analysis for those interested. Clustering algorithms are implemented in scikit-learn very similarly as classification or regression algorithms. So you should be able to solve this on your own quite easily. \n",
    " \n",
    " \n",
    " Last theory lecture also included hyperparameter optimization, but this was already covered during the practical class of Week 3.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\"> Missing data </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_1\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "For reproducibility purposes we will always use random_state=0 in this practical. Don't forget to assign this parameter in your functions if it is needed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the practical we will work with the *communities* dataset, which has a large number of features, as well as missing values. It contains demographic information about countries in the United States, and the goal is to predict the number of violent crimes (attribute A122)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the dataset and extract the feature we want to predict\n",
    "data_dir = \"/data/gent/shared/000/gvo00070/BigDataScience2023/\"\n",
    "df = pd.read_csv(data_dir + 'communities.psv', sep = '|', na_values = '?')\n",
    "X = df.drop(\"A122\", axis=1)\n",
    "y = df[\"A122\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3> Exercise:</h3>\n",
    "<ol> \n",
    " <li>Divide the dataset into a training (80%) and test (20%) set\n",
    " <li>For the training set, count the number of attributes with missing values as well as the percentage of missing values\n",
    "</ol> \n",
    "\n",
    "**HINT**: look at how the missing values are represented in the data (their datatype)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of attributes with missing values\n",
    "attributes_with_missing = np.sum(np.any(np.isnan(X_train), axis=0))\n",
    "print(\"Attributes with missing values:\", attributes_with_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the percentage of missing values\n",
    "missing_rate = np.count_nonzero(np.isnan(X_train))/np.prod(X_train.shape)\n",
    "print(\"Percentage of missing values:\", missing_rate*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3> Exercise:</h3>\n",
    "There are multiple single imputation strategies for missing data implemented in scikit-learn. Perform the following strategies:\n",
    "<ol> \n",
    " <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\">Simple imputation</a> with the mean\n",
    " <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\">Simple imputation</a> with the median\n",
    " <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer\">Nearest Neighbor imputation</a> \n",
    "</ol> \n",
    "Fit for each strategy a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">linear regression model </a>and evaluate the model's performance with the R-squared metric.\n",
    "\n",
    "Each of these methods has a parameter <i>add_indicator</i>. Find out what the purpose of this parameter is and evaluate it's influence on the prediction performance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *add_indicator* parameters allows to use information about which values are missing as an extra feature. For a single array this would mean that the algorithm would add a column to the data that indicates missingness of a data point in the non-transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Declare the imputation methods\n",
    "imputation_methods_no_ind = {\n",
    "    'mean imputation': SimpleImputer(strategy=\"mean\", add_indicator=False),\n",
    "    'median imputation': SimpleImputer(strategy=\"median\", add_indicator=False),\n",
    "    'neighbours imputation': KNNImputer( add_indicator=False),\n",
    "}\n",
    "imputation_methods_ind = {\n",
    "    'mean imputation': SimpleImputer(strategy=\"mean\", add_indicator=True),\n",
    "    'median imputation': SimpleImputer(strategy=\"median\", add_indicator=True),\n",
    "    'neighbours imputation': KNNImputer( add_indicator=True),\n",
    "}\n",
    "\n",
    "# Declare the model\n",
    "clf = LinearRegression()\n",
    "\n",
    "# Run the imputators + models\n",
    "## Without inidicators\n",
    "scores_no_ind = np.zeros(3)\n",
    "methods = []\n",
    "i = 0\n",
    "for method, imputer in imputation_methods_no_ind.items():\n",
    "    imputer.fit(X_train)\n",
    "    X_train_imp = imputer.transform(X_train)\n",
    "    X_test_imp = imputer.transform(X_test)\n",
    "    \n",
    "    clf.fit(X_train_imp, y_train)\n",
    "    scores_no_ind[i] = clf.score(X_test_imp, y_test)\n",
    "    methods.append(method)\n",
    "    i += 1\n",
    "    \n",
    "## With indicators\n",
    "scores_ind = np.zeros(3)\n",
    "i = 0\n",
    "for method, imputer in imputation_methods_ind.items():\n",
    "    imputer.fit(X_train)\n",
    "    X_train_imp = imputer.transform(X_train)\n",
    "    X_test_imp = imputer.transform(X_test)\n",
    "    \n",
    "    clf.fit(X_train_imp, y_train)\n",
    "    scores_ind[i] = clf.score(X_test_imp, y_test)\n",
    "    i += 1\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(methods, scores_ind,  label = 'With indicator' )\n",
    "ax.scatter(methods, scores_no_ind,  label = 'Without indicator' )\n",
    "ax.legend()\n",
    "plt.ylabel('Coefficient of determination')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\"> Intermezzo - Scaling </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"intermezzo\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the lectures, scaling/normalization is oftentimes quite important as it can heavily influence prediction results. Therefore,  **it is almost always recommended to perform some form of normalization or scaling of the data**, especially when regularization, distance-based classifiers or classifiers with normality/distribution assumptions are implemented. In scikit-learn you have the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\">StandardScaler</a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler\">RobustScaler</a> available for respectively standardizaton and robust feature scaling. \n",
    "\n",
    "But do note, that in order to prevent data leakage, **scaling can only be based on the training dataset characteristics**. As the model is trained on scaled features, for a proper prediction the test data will also need to be on the same scale. But the test data can only be scaled based on the training data's characteristics, not on its own characteristics. *E.g. for standardization, the mean and variance of the training data will be calculated and used for transformation of the entire dataset.*\n",
    "\n",
    "Worthwhile to mention here is also the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\"> pipeline constructor </a> in scikit-learn. This constructor allows you to sequentially perform multiple scikit-learn transform actions and one final estimation action.\n",
    "\n",
    "So for instance say you would want to perform a scaling transformation and then fit a linear SVM model, with the help of the pipeline constructor this can be efficiently implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Construct a toy classification dataset\n",
    "X, y = make_classification(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)\n",
    "\n",
    "# Construct a pipeline with a scaler and a svc step\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])  \n",
    "\n",
    "# This pipeline can be used as any other constructor in scikit-learn\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# The pipeline will inherit the scoring method from the final estimator\n",
    "# Scaling will only be trained on the train dataset, but the test data will also be transformed\n",
    "pipe.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can access the independent steps\n",
    "pipe['scaler'].mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\"> Dimensionality reduction </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_2\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is an important tool in data science, it performs a transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data. These properties vary with each algorithm.\n",
    "\n",
    "**Principal component analysis (PCA)** might be the most known dimensionality reduction method. It's goal is to project the data to a lower dimension coordinate system by preserving as much variance in the data as possible. The (new) axes of the lower dimensional data are then called principal components (PCs)\n",
    "\n",
    "**Linear Discriminant analysis** (introduced during week 3 as a classifier) can also be used as a dimensionality reduction method, but its objective for dimension reduction is not maximizing variance, but finding the lower dimensional space that best separates the classes present in the data.\n",
    "\n",
    "![CV](./PCA_LDA.png)\n",
    "\n",
    "Many other exist, such as **UMAP**,  **t-SNE**, **MDS** etc.\n",
    "\n",
    "Dimensionality reduction is used within data science for two purposes:\n",
    "<ol>\n",
    "    <li> <i>Data visualisation</i>\n",
    "    <li> <i>Feature reduction</i>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the practical we will make use of *the Winconsin Diagnostic Breast Cancer (wdbc)* dataset that can be used to classify tumors as benign or malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/data/gent/shared/000/gvo00070/BigDataScience2023/\"\n",
    "df = pd.read_csv(data_dir + 'wdbc.psv', sep = '|', na_values = '?')\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3> Exercise:</h3>\n",
    "<ol>\n",
    "    <li> Perform <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\">5-fold CV</a>\n",
    "    <li> Scale the data \n",
    "    <li> Perform a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">PCA</a> transformation and make sure 85% of the variation is retained in the lower dimension\n",
    "    <li> Classify the data with a <a href=\"https://sci>kit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">Decision tree classifier</a>\n",
    "    <li> Evaluate the model with the mean <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\">accuracy score</a>\n",
    "    <li> Plot the first PCs against each other colored by class label (<b>HINT</b>: you can access the fitted objects in the pipeline)\n",
    "<ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Declare the K-fold constructor\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# initialize an array to store the accuracy per fold\n",
    "acc = np.zeros(5)\n",
    "\n",
    "# Perform 5-fold CV\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    # Per split, construct the training and test data\n",
    "    X_train = X.values[train_index]\n",
    "    X_test= X.values[test_index]\n",
    "    y_train = y.values[train_index]\n",
    "    y_test = y.values[test_index]\n",
    "    \n",
    "    # Declare the pipeline constructor\n",
    "    pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('dimensionality_reduction', PCA(n_components=0.85)),\n",
    "    ('classification', DecisionTreeClassifier()),\n",
    "    ]) \n",
    "    \n",
    "    # Fit and evaluate the pipeline\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    scores = pipe.predict_proba(X_test)\n",
    "\n",
    "    # Measure and store the accuracy per fold\n",
    "    acc[i] = accuracy_score(y_test, preds)\n",
    "\n",
    "\n",
    "print('mean accuracy score', np.mean(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this part we will use the model constructed in the last fold that is still present in working environment of the notebook\n",
    "# Scale the training data\n",
    "X_train_scaled = pipe['scaler'].transform(X_train)\n",
    "\n",
    "# Extract the first three dimensions of the transformed training data\n",
    "pc_0 = pipe['dimensionality_reduction'].transform(X_train_scaled)[:,0]\n",
    "pc_1 = pipe['dimensionality_reduction'].transform(X_train_scaled)[:,1]\n",
    "pc_2 = pipe['dimensionality_reduction'].transform(X_train_scaled)[:,2]\n",
    "\n",
    "# Scale the test data\n",
    "X_test_scaled = pipe['scaler'].transform(X_test)\n",
    "\n",
    "# Extract the first three dimensions of the transformed test data\n",
    "pc_0_test = pipe['dimensionality_reduction'].transform(X_test_scaled)[:,0]\n",
    "pc_1_test = pipe['dimensionality_reduction'].transform(X_test_scaled)[:,1]\n",
    "pc_2_test = pipe['dimensionality_reduction'].transform(X_test_scaled)[:,2]\n",
    "\n",
    "# Indicate a coloring dictionary\n",
    "color = {\n",
    "    'M': 'red',\n",
    "    'B': 'blue'\n",
    "}\n",
    "color_groups = [color[i] for i in y_train]\n",
    "color_groups_test = [color[i] for i in y_test]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1,2, figsize = (15,5))\n",
    "ax[0].scatter(pc_0, pc_1, c = color_groups)\n",
    "ax[0].set_title('PC0-PC1 Train')\n",
    "ax[0].set_xlabel('PC 0')\n",
    "ax[0].set_ylabel('PC 1')\n",
    "\n",
    "ax[1].scatter(pc_0_test, pc_1_test, c = color_groups_test)\n",
    "ax[1].set_title('PC0-PC1 Test')\n",
    "ax[1].set_xlabel('PC 0')\n",
    "ax[1].set_ylabel('PC 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize = (15,5))\n",
    "ax[0].scatter(pc_1, pc_2, c = color_groups)\n",
    "ax[0].set_title('PC1-PC2 Train')\n",
    "ax[0].set_xlabel('PC 1')\n",
    "ax[0].set_ylabel('PC 2')\n",
    "\n",
    "ax[1].scatter(pc_1_test, pc_2_test, c = color_groups_test)\n",
    "ax[1].set_title('PC1-PC2 Test')\n",
    "ax[1].set_xlabel('PC 1')\n",
    "ax[1].set_ylabel('PC 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\"> Optional - Clustering </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"Optional\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3> Exercise:</h3>\n",
    "<ol>\n",
    "    <li> Split the data in a training (80%) and test (20%) set\n",
    "    <li> Classify the data with a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\">Kmeans</a> and test out some different <i>n_cluster</i> parameters \n",
    "    <li> Visualize the predictions of the different models\n",
    "<ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some toy data\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=5000,\n",
    "  centers=5,\n",
    "  cluster_std=2,\n",
    "  random_state=42\n",
    " )\n",
    "plt.scatter(X[:,0], X[:,1], c = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Declare the n_cluster paramters \n",
    "params_n_clusters = [2,3,4,5,6,7]\n",
    "\n",
    "# initialize the figure and make the individual axis easily accessible in the for loop\n",
    "fig, axes = plt.subplots(2,3, figsize=(15,10))\n",
    "coords = [[0,0], [0,1], [0,2], [1,0], [1,1], [1,2]]\n",
    "\n",
    "# loop over the parameters, fit the model, predict the classes and visualize the predictions in a subplot\n",
    "for i in range(0,len(params_n_clusters)):\n",
    "    clf = KMeans(n_clusters = params_n_clusters[i])\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    \n",
    "    ax = axes[coords[i][0], coords[i][1]]\n",
    "    ax.scatter(X_test[:,0], X_test[:,1], c = preds)\n",
    "    ax.set_title('n clusters: ' + str(params_n_clusters[i]) )\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
