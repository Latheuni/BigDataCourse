{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical session: Building interactive apps with Streamlit\n",
        "\n",
        "[Streamlit](https://streamlit.io/) is a faster way to build and share data apps in Python. You can run your script in the command line as follows:\n",
        "\n",
        "`streamlit run your_script.py [-- script args]`\n",
        "\n",
        "If installed and run locally, it is similar to a Jupyter notebook in that your app can be viewed in a browser, and the server can be stopped with `Ctrl+C`.\n",
        "\n",
        "As we are running streamlit within Colab for this practical session, we will use Cloudflare to host the server that runs our app."
      ],
      "metadata": {
        "id": "pwDuxJqNj9ST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Set up Streamlit and Cloudfare\n",
        "\n",
        "As a first step, we will install Streamlit and Cloudfare."
      ],
      "metadata": {
        "id": "MphGjE9SUYoT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2HBvLfzDdJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2743fa-f575-45f7-df92-38efc3611793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h--2024-05-15 14:04:53--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2024.4.1/cloudflared-linux-amd64 [following]\n",
            "--2024-05-15 14:04:53--  https://github.com/cloudflare/cloudflared/releases/download/2024.4.1/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/338c4db6-d448-42d7-9218-0662e513e932?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240515%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240515T140332Z&X-Amz-Expires=300&X-Amz-Signature=766d57922e1e6bfec06cc587722ca561573e77c0497ad5ea704ca8d094367256&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=106867604&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-05-15 14:04:53--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/338c4db6-d448-42d7-9218-0662e513e932?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240515%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240515T140332Z&X-Amz-Expires=300&X-Amz-Signature=766d57922e1e6bfec06cc587722ca561573e77c0497ad5ea704ca8d094367256&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=106867604&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36530984 (35M) [application/octet-stream]\n",
            "Saving to: ‘cloudflared-linux-amd64’\n",
            "\n",
            "cloudflared-linux-a 100%[===================>]  34.84M   187MB/s    in 0.2s    \n",
            "\n",
            "2024-05-15 14:04:54 (187 MB/s) - ‘cloudflared-linux-amd64’ saved [36530984/36530984]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit\n",
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check if we can get the Cloudfare tunnel to work. We will write a simple app to the file `Home.py`. This app displays the square of the number on the slider."
      ],
      "metadata": {
        "id": "jB2TEUO1Dv4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Home.py\n",
        "import streamlit as st\n",
        "\n",
        "x = st.slider('Select a value')\n",
        "st.write(x, 'squared is', x * x)"
      ],
      "metadata": {
        "id": "tRmnRDFNDfE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228c7f53-e2e6-4f4a-dbc6-030513229ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Home.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the `app.py` file. If running locally, type the code (without the exclamation mark) in the command line, and a new browser should pop up."
      ],
      "metadata": {
        "id": "w0p9cyyQEJxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/Home.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "JWIumRZ9EIth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When running on Google Colab, we have to open a Cloudflare tunnel in the background that displays things we have running on port 8501, which is the port that Streamlit applications are run on by default. Try clicking the link below to see whether your app is successfully hosted there. If there are errors, you may have to install streamlit locally instead.\n",
        "\n",
        "Also note that since we are running the tunnel without a Cloudflare account, we have no uptime guarantee."
      ],
      "metadata": {
        "id": "cao-EiIbEWRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open Cloudflare tunnel\n",
        "!nohup /content/cloudflared-linux-amd64 tunnel --url http://localhost:8501 &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itS3GphAqjTV",
        "outputId": "2d0bfe34-8f19-4fc9-dde7-23c8156b9678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get link to Cloudflare tunnel\n",
        "!grep -o 'https://.*\\.trycloudflare.com' nohup.out | head -n 1 | xargs -I {} echo \"Your tunnel url {}\" # Display the url where your app will run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aZGHP99kwwF",
        "outputId": "58aed9f8-c745-4738-d92a-f52da852b1f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your tunnel url https://achievement-fig-analysts-una.trycloudflare.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT:** Only run `streamlit run` once, otherwise you will be running several instances on different ports (by default, the next instances will be on port 8502, 8503, and so on). This may cause your app to not refresh anymore. In that case, you will have to kill all streamlit processes and run it again, using the `kill` command.\n",
        "\n",
        "To do this, first check the PID of all streamlit processes."
      ],
      "metadata": {
        "id": "bbiOiwsvFDN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWxQrZuFCr-S",
        "outputId": "1cd3edf6-dc2b-4f29-8476-b1283ae1b680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    PID TTY          TIME CMD\n",
            "      1 ?        00:00:00 docker-init\n",
            "      6 ?        00:00:18 node\n",
            "     17 ?        00:00:03 oom_monitor.sh\n",
            "     19 ?        00:00:00 run.sh\n",
            "     20 ?        00:00:02 kernel_manager_\n",
            "     23 ?        00:00:00 tail\n",
            "     62 ?        00:00:07 python3 <defunct>\n",
            "     63 ?        00:00:01 colab-fileshim.\n",
            "     81 ?        00:00:09 jupyter-noteboo\n",
            "     82 ?        00:00:07 dap_multiplexer\n",
            "    190 ?        00:00:33 python3\n",
            "    217 ?        00:00:16 python3\n",
            "    775 ?        00:00:14 streamlit\n",
            "   1710 ?        00:00:08 cloudflared-lin\n",
            "   8238 ?        00:00:03 language_servic\n",
            "   8243 ?        00:07:06 node\n",
            "  25576 ?        00:00:00 sleep\n",
            "  25577 ?        00:00:00 ps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there is more than one streamlit process, do:\n",
        "`!kill -9 {PID}`."
      ],
      "metadata": {
        "id": "pH57wthIC41B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Building a small chat app\n",
        "\n",
        "We will start off with a simple chat application that simply echoes what the user input is. When you run the cell below, the file `Home.py` will be overwritten. If you switch to the tab where you have your app opened, the top right should say \"$i$ Source file changed.\", to which you can click \"Rerun\". If not, you can click the three dots in the corner and press \"Rerun\", or just press R (while not in the message box)."
      ],
      "metadata": {
        "id": "GcoIhKTApLdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Home.py\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"Echo Bot\")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"What is up?\"):\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"user\").markdown(prompt)\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    response = prompt\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3j9YrlwDoNY",
        "outputId": "2d092d8f-defa-4fe4-910f-d535c14a84f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Home.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation (also check the API [here](https://docs.streamlit.io/develop/api-reference)):**\n",
        "*  `st.session_state.messages` is a list that stores the chat history ([session state](https://docs.streamlit.io/develop/api-reference/caching-and-state/st.session_state) is a way to store variable between reruns). Each entry in the list will be a dictionary with the following keys: role (the author of the message), and content (the message content). The author can be either \"user\" or \"assistant\" to enable preset styling and avatars. You can pass in a custom string to use as the author name, but this will not be shown in the UI.\n",
        "*  `st.chat_input`: displays a chat input widget so the user can type in a message. The returned value is the user's input, which is `None` if the user hasn't sent a message yet. You can also pass in a default prompt to display in the input widget.\n",
        "*  `st.chat_message`: inserts a multi-element chat message container into your app. The first parameter is the name of the message author, and the returned container can contain any Streamlit element, including charts, tables, text, and more. To add elements to the returned container, you can use `with` notation.\n",
        "* `st.markdown`: writes text formatted as Markdown."
      ],
      "metadata": {
        "id": "9SHve1_7V6E1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🗒 **TASK** 🗒\n",
        "\n",
        "Change the reply of the chatbot assistant to whatever you'd like (e.g., add a random emoji at the end, a reverse of the user's input, ...)."
      ],
      "metadata": {
        "id": "qpDZJuuTbL-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Building a Llama 3 chatbot\n",
        "\n",
        "Next, we will make use of Llama 3 to generate the responses to user queries. We will use the Llama 3 model hosted on Replicate, a service that lets you run machine learning models with a cloud API. To do this, go to [replicate.com](https://replicate.com) and log in with your GitHub account. Then, copy your token from **Account settings > API tokens**.\n",
        "\n",
        "Unfortunately, you will only be able to test out a few queries before you are paywalled, but it should be enough to try it out!"
      ],
      "metadata": {
        "id": "u-gmkvTKHIaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install replicate"
      ],
      "metadata": {
        "id": "l3XcoL-EdQuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3314003c-c95d-4785-ad22-b757806cc70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting replicate\n",
            "  Downloading replicate-0.26.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m924.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx<1,>=0.21.0 (from replicate)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from replicate) (24.0)\n",
            "Requirement already satisfied: pydantic>1.10.7 in /usr/local/lib/python3.10/dist-packages (from replicate) (2.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (4.11.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.21.0->replicate)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.21.0->replicate)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (2.18.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.2.1)\n",
            "Installing collected packages: h11, httpcore, httpx, replicate\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 replicate-0.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "your_token = ... # paste your token here\n",
        "\n",
        "import os\n",
        "if not os.path.exists('.streamlit/'):\n",
        "    # Create the folder\n",
        "    os.makedirs('.streamlit/')\n",
        "\n",
        "file = open('.streamlit/secrets.toml', 'w')\n",
        "print('REPLICATE_API_TOKEN = \"{}\"'.format(your_token), file = file)\n",
        "file.close()\n"
      ],
      "metadata": {
        "id": "jGjJyxXBcm_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Home.py\n",
        "import streamlit as st\n",
        "import replicate\n",
        "import os\n",
        "\n",
        "# App title\n",
        "st.set_page_config(page_title=\"🦙💬 Llama 3 Chatbot\")\n",
        "\n",
        "# Add sidebar\n",
        "with st.sidebar:\n",
        "    st.title('🦙💬 Llama 3 Chatbot')\n",
        "\n",
        "    # Check token\n",
        "    if 'REPLICATE_API_TOKEN' in st.secrets:\n",
        "        replicate_api = st.secrets['REPLICATE_API_TOKEN']\n",
        "        if not (replicate_api.startswith('r8_') and len(replicate_api)==40):\n",
        "            st.warning('Your credentials seem to be incorrect!', icon='⚠️')\n",
        "        else:\n",
        "            st.success('API key success!', icon='✅')\n",
        "\n",
        "    os.environ['REPLICATE_API_TOKEN'] = replicate_api\n",
        "\n",
        "    # Add hyperparameter sliders\n",
        "    st.subheader('Hyperparameters')\n",
        "    st.session_state['temperature'] = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "    st.session_state['top_p'] = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "    st.session_state['max_tokens'] = st.sidebar.slider('max_tokens', min_value=32, max_value=128, value=120, step=8)\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "# Clear chat history button\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Function for generating LLAMA3 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\n",
        "def generate_llama3_response(prompt_input, temperature, top_p, max_tokens):\n",
        "    pre_prompt = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n",
        "    for dict_message in st.session_state.messages:\n",
        "        if dict_message[\"role\"] == \"user\":\n",
        "            pre_prompt += \"User: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "        else:\n",
        "            pre_prompt += \"Assistant: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "    output = replicate.run('meta/meta-llama-3-70b-instruct',\n",
        "                           input={\"prompt\": f\"{pre_prompt} {prompt_input} Assistant: \",\n",
        "                                  \"temperature\":temperature, \"top_p\":top_p, \"max_tokens\":max_tokens, \"repetition_penalty\":1})\n",
        "    return output\n",
        "\n",
        "# User-provided prompt\n",
        "if prompt := st.chat_input(disabled=not replicate_api):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response if last message is not from assistant\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = generate_llama3_response(prompt, st.session_state['temperature'], st.session_state['top_p'], st.session_state['max_tokens'])\n",
        "            # Type out response word by word\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "            for item in response:\n",
        "                full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)"
      ],
      "metadata": {
        "id": "7JJUXEFfdKfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a33f8b-ae22-4be1-86c2-c882856dc40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Home.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above includes a sidebar where we allow users to tune [hyperparameters](https://replicate.com/meta/meta-llama-3-70b-instruct/api/schema) of the Llama model. We include three hyperparameters:\n",
        "\n",
        "* `temperature`: The value used to modulate the next token probabilities.\n",
        "* `top_p`: A probability threshold for generating the output. If < 1.0, only keep the top tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751).\n",
        "* `max_length`: The maximum number of words the model should generate as output.\n",
        "\n",
        "The code was taken from this [blog post]((https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)) with slight modifications."
      ],
      "metadata": {
        "id": "khy1AbX6cmo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Building a webpage for your Deep learning analysis\n",
        "\n",
        "In this section, we will turn your deep learning analysis from Practical session 9 into a web application. We will create three pages: the home page, the data loading and preprocessing page, and the model training page. At the bottom of each code cell, you will find the code that is relevant to create the app."
      ],
      "metadata": {
        "id": "GfbTnNwBfgVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('pages/'):\n",
        "    # Create the folder to store subpages\n",
        "    os.makedirs('pages/')"
      ],
      "metadata": {
        "id": "ZJcgwMHMfzsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Home.py\n",
        "import streamlit as st\n",
        "from tensorflow.python.client import device_lib\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Results practical session deep Learning\",\n",
        "    page_icon = \"https://cdn-icons-png.flaticon.com/512/6037/6037989.png\"\n",
        ")\n",
        "st.sidebar.header(\"Analysis steps\")\n",
        "st.image(\"https://cdn-icons-png.flaticon.com/512/6037/6037989.png\")\n",
        "st.markdown(\"# Malaria cell classification \")\n",
        "st.markdown('''Malaria is a blood disease caused by the Plasmodium parasite, and is transmitted through the bite of the female Anopheles mosquito. The disease is mostly diagnosed by counting parasitized blood cells in a blood smear under a microscope. However, manual cell counting is an exhausting, error-prone procedure. This can negatively affect the quality of the diagnosis. Especially in resource-constrained regions of the world, difficult working conditions lead to poor diagnosis quality.''')\n",
        "st.markdown('''In this practical session we will develop a deep learning pipeline that will aid in improving malaria diagnosis by automating infected cell counting. To this end we will use the Malaria cell dataset to train a neural network that predicts a cell's infection state based on a microscopy image of it. The microscopy images were acquired using a smartphone attached to a small portable microscope.''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxO7BMHHENgN",
        "outputId": "8aaa80f5-1bcf-49a5-ae58-cdb9b2b79729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Home.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/1_Data.py\n",
        "import streamlit as st\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "####### FUNCTIONS #######\n",
        "def select_device(prefer_gpu=True):\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
        "    if (len(gpus) > 0) and prefer_gpu:\n",
        "        return gpus[0]\n",
        "    else:\n",
        "        return [x.name for x in local_device_protos if x.device_type == 'CPU'][0]\n",
        "\n",
        "def initialize_data():\n",
        "  device = select_device(prefer_gpu=True)\n",
        "  builder = tfds.builder('malaria')\n",
        "  builder.download_and_prepare()\n",
        "\n",
        "  with tf.device(device):\n",
        "    train_ds, test_ds = (\n",
        "        builder.as_dataset(as_supervised=True, split=\"train[:80%]\"),\n",
        "        builder.as_dataset(as_supervised=True, split=\"train[-20%:]\")\n",
        "    )\n",
        "  return train_ds, test_ds\n",
        "\n",
        "def plot_10_images(train_ds):\n",
        "  n_images = 10\n",
        "\n",
        "  # select some images from the dataset\n",
        "  images = train_ds.take(n_images)\n",
        "\n",
        "  fig, axes = plt.subplots(1, n_images, figsize=(20, 5), dpi=100)\n",
        "  for ax, (image, label) in zip(axes, images):\n",
        "      ax.imshow(image)\n",
        "      ax.set_title(\"Healthy\" if label.numpy() == 1 else \"Malaria\")\n",
        "\n",
        "  return(fig)\n",
        "\n",
        "def resize_images(image, label, image_width=40, image_height=40):\n",
        "    \"\"\"\n",
        "    Resizes all images in a dataset to a uniform width and height.\n",
        "    Also casts the images and labels to the float32 data type.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        tf.cast(tf.image.resize_with_pad(tf.image.resize(image, (image_width, image_height)), image_width, image_height), tf.float32),\n",
        "        tf.cast(label, tf.float32)\n",
        "    )\n",
        "\n",
        "def minmax_normalization(image, label):\n",
        "    \"\"\"Normalizes the pixel value range of an image to [0, 1] by dividing each pixel by the maximum value in each channel.\"\"\"\n",
        "    return (\n",
        "        image / tf.math.reduce_max(tf.reshape(image, [-1, image.shape[-1]]), axis=0), # divide each pixel in the image by the maximum value in each channel (R, G, and B)\n",
        "        label\n",
        "    )\n",
        "\n",
        "####### APP #######\n",
        "st.title(\"Data preprocessing and inspection\")\n",
        "st.sidebar.header(\"Data inspection\")\n",
        "with st.spinner(\"Loading in the data\"):\n",
        "  train_ds, test_ds = initialize_data()\n",
        "  fig = plot_10_images(train_ds)\n",
        "st.write('Data loading done, example of training images')\n",
        "st.pyplot(fig)\n",
        "st.write('\\n')\n",
        "\n",
        "with st.spinner(\"Resizing the data and applying min-max scaling\"):\n",
        "  train_ds = train_ds.map(resize_images)\n",
        "  test_ds = test_ds.map(resize_images)\n",
        "\n",
        "  train_ds = train_ds.map(minmax_normalization)\n",
        "  test_ds = test_ds.map(minmax_normalization)\n",
        "st.write('Data preprocessing done, example of preprocessed training images')\n",
        "st.pyplot(fig)\n",
        "st.write('\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY74Tt7tS5PO",
        "outputId": "3b7ea8e8-d8f8-4dd9-8e37-ae6420a73263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pages/1_Data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/2_Model.py\n",
        "import streamlit as st\n",
        "from tensorflow.python.client import device_lib\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "####### FUNCTIONS #######\n",
        "def select_device(prefer_gpu=True):\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
        "    if (len(gpus) > 0) and prefer_gpu:\n",
        "        return gpus[0]\n",
        "    else:\n",
        "        return [x.name for x in local_device_protos if x.device_type == 'CPU'][0]\n",
        "\n",
        "def resize_images(image, label, image_width=40, image_height=40):\n",
        "    \"\"\"\n",
        "    Resizes all images in a dataset to a uniform width and height.\n",
        "    Also casts the images and labels to the float32 data type.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        tf.cast(tf.image.resize_with_pad(tf.image.resize(image, (image_width, image_height)), image_width, image_height), tf.float32),\n",
        "        tf.cast(label, tf.float32)\n",
        "    )\n",
        "\n",
        "\n",
        "def minmax_normalization(image, label):\n",
        "    \"\"\"Normalizes the pixel value range of an image to [0, 1] by dividing each pixel by the maximum value in each channel.\"\"\"\n",
        "    return (\n",
        "        image / tf.math.reduce_max(tf.reshape(image, [-1, image.shape[-1]]), axis=0), # divide each pixel in the image by the maximum value in each channel (R, G, and B)\n",
        "        label\n",
        "    )\n",
        "\n",
        "def initialize_data():\n",
        "  device = select_device(prefer_gpu=True)\n",
        "  builder = tfds.builder('malaria')\n",
        "  builder.download_and_prepare()\n",
        "\n",
        "  with tf.device(device):\n",
        "    train_ds, test_ds = (\n",
        "        builder.as_dataset(as_supervised=True, split=\"train[:80%]\"),\n",
        "        builder.as_dataset(as_supervised=True, split=\"train[-20%:]\")\n",
        "    )\n",
        "  return train_ds, test_ds\n",
        "def select_device(prefer_gpu=True):\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
        "    if (len(gpus) > 0) and prefer_gpu:\n",
        "        return gpus[0]\n",
        "    else:\n",
        "        return [x.name for x in local_device_protos if x.device_type == 'CPU'][0]\n",
        "\n",
        "\n",
        "def run_model(train_ds, test_ds, epoch, batch_size):\n",
        "  device = select_device(prefer_gpu=True)\n",
        "  with tf.device(device):\n",
        "    classifier = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(filters=2, kernel_size=3, strides=(1,1), padding=\"same\"),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "        tf.keras.layers.ReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=2, kernel_size=3, strides=(2,2), padding=\"same\"),\n",
        "        tf.keras.layers.ReLU(),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(10),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.ReLU(),\n",
        "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "    bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
        "    sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "    accuracy_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "\n",
        "    with tf.device(device):\n",
        "      classifier.compile(optimizer=sgd_optimizer, loss=bce_loss, metrics=[accuracy_metric])\n",
        "\n",
        "    train_len = len(list(train_ds))\n",
        "    test_len = len(list(test_ds))\n",
        "    steps_per_epoch = train_len//batch_size\n",
        "    test_steps = test_len//batch_size\n",
        "\n",
        "    with tf.device(device):\n",
        "      history = classifier.fit(\n",
        "          train_ds.batch(batch_size).repeat(), # on which data to we want to train\n",
        "          epochs=epoch, # how many epochs do we want to run\n",
        "          steps_per_epoch=steps_per_epoch, # how many steps are in one epoch\n",
        "          validation_data=test_ds.batch(batch_size).repeat(), # what test data do we want to use\n",
        "          validation_steps=test_steps # how many steps do we need to take when testing\n",
        "      )\n",
        "  return(history)\n",
        "\n",
        "####### APP #######\n",
        "st.title('Model')\n",
        "st.sidebar.header(\"Model training and inspection\")\n",
        "\n",
        "# Data loading\n",
        "train_ds, test_ds = initialize_data()\n",
        "train_ds = train_ds.map(resize_images)\n",
        "test_ds = test_ds.map(resize_images)\n",
        "train_ds = train_ds.map(minmax_normalization)\n",
        "test_ds = test_ds.map(minmax_normalization)\n",
        "\n",
        "with st.form(\"my_form\"):\n",
        "  st.write(\"Before starting the analysis, please choose the following values\")\n",
        "  epoch_val = st.slider(\"Number of epochs\")\n",
        "  batch_size = st.selectbox(\"Select batch size\", (128,256), placeholder=\"Please choose a batch size\")\n",
        "  submitted = st.form_submit_button(\"Submit\")\n",
        "\n",
        "if submitted:\n",
        "  with st.spinner(\"Training the model, please note this can take a while\"):\n",
        "    history = run_model(train_ds, test_ds, epoch_val, batch_size)\n",
        "\n",
        "  plt.figure(figsize=(15, 8))\n",
        "  plt.plot(history.epoch, history.history[\"binary_accuracy\"], color=\"red\")\n",
        "  plt.plot(history.epoch, history.history[\"val_binary_accuracy\"], color=\"blue\")\n",
        "  st.pyplot(plt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knxLkFecUO4Y",
        "outputId": "98ad9cfd-e1e0-4ce8-c32a-3f2a0515d667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pages/2_Model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🗒 **TASK** 🗒\n",
        "\n",
        "Allow the users to select between the SGD and Adam optimizer."
      ],
      "metadata": {
        "id": "qRMgWVIbrzQg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NI3Q8t9lr8pD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}